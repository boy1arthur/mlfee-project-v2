---
title: 'ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì™„ì „ ê°€ì´ë“œ'
date: '2025-09-03'
excerpt: 'ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ë¶€í„° F1-Score, AUC-ROCê¹Œì§€ ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ í•œë²ˆì— ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.'
tags: ['ë¨¸ì‹ ëŸ¬ë‹', 'í‰ê°€ì§€í‘œ', 'ëª¨ë¸ì„±ëŠ¥']
---

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ì˜ ëª¨ë“  ê²ƒ

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í–ˆë‹¤ë©´, ì´ì œ ê·¸ ì„±ëŠ¥ì„ ì •í™•íˆ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì–´ë–¤ ì§€í‘œë¥¼ ì‚¬ìš©í•´ì•¼ í• ì§€ í—·ê°ˆë¦¬ì‹œë‚˜ìš”? ì˜¤ëŠ˜ì€ ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

## 1. ë¶„ë¥˜(Classification) ì„±ëŠ¥ í‰ê°€

### ê¸°ë³¸ ê°œë…: Confusion Matrix

ëª¨ë“  ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œì˜ ì¶œë°œì ì€ **í˜¼ë™í–‰ë ¬(Confusion Matrix)**ì…ë‹ˆë‹¤:

```python
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
X = np.random.rand(1000, 4)
y = np.random.randint(0, 2, 1000)

# ëª¨ë¸ í›ˆë ¨
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
```

### í•µì‹¬ í‰ê°€ ì§€í‘œë“¤

#### 1) ì •í™•ë„ (Accuracy)
ê°€ì¥ ì§ê´€ì ì¸ ì§€í‘œì´ì§€ë§Œ, **ë¶ˆê· í˜• ë°ì´í„°**ì—ì„œëŠ” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```python
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"ì •í™•ë„: {accuracy:.4f}")

# ë¶ˆê· í˜• ë°ì´í„° ì˜ˆì‹œ
# 99% ì •ìƒ, 1% ì´ìƒ -> ëª¨ë“  ê±¸ ì •ìƒì´ë¼ê³  í•´ë„ 99% ì •í™•ë„!
```

#### 2) ì •ë°€ë„ (Precision)
"ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì–‘ì„±ì˜ ë¹„ìœ¨"

```python
from sklearn.metrics import precision_score

precision = precision_score(y_test, y_pred)
print(f"ì •ë°€ë„: {precision:.4f}")

# ì–¸ì œ ì¤‘ìš”í•œê°€?
# - ìŠ¤íŒ¸ ë©”ì¼ íƒì§€: ì •ìƒ ë©”ì¼ì„ ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜í•˜ë©´ ì•ˆ ë¨
# - ì˜ë£Œ ì§„ë‹¨: ê±´ê°•í•œ ì‚¬ëŒì„ í™˜ìë¡œ ì§„ë‹¨í•˜ë©´ ë¶ˆí•„ìš”í•œ ì¹˜ë£Œ
```

#### 3) ì¬í˜„ìœ¨ (Recall)
"ì‹¤ì œ ì–‘ì„± ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨"

```python
from sklearn.metrics import recall_score

recall = recall_score(y_test, y_pred)
print(f"ì¬í˜„ìœ¨: {recall:.4f}")

# ì–¸ì œ ì¤‘ìš”í•œê°€?
# - ì•” ì§„ë‹¨: ì•” í™˜ìë¥¼ ë†“ì¹˜ë©´ ì•ˆ ë¨
# - ì‚¬ê¸° íƒì§€: ì‚¬ê¸° ê±°ë˜ë¥¼ ë†“ì¹˜ë©´ í° ì†ì‹¤
```

#### 4) F1-Score
ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· ìœ¼ë¡œ, **ë¶ˆê· í˜•í•œ ìƒí™©**ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.

```python
from sklearn.metrics import f1_score

f1 = f1_score(y_test, y_pred)
print(f"F1-Score: {f1:.4f}")

# F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

## 2. ê³ ê¸‰ í‰ê°€ ì§€í‘œ

### ROC Curveì™€ AUC

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# í™•ë¥  ì˜ˆì¸¡ê°’ í•„ìš”
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ROC Curve ê³„ì‚°
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

print(f"AUC-ROC: {roc_auc:.4f}")
```

### Precision-Recall Curve

ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ROCë³´ë‹¤ **PR Curve**ê°€ ë” ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)
ap_score = average_precision_score(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall_vals, precision_vals, color='blue', lw=2,
         label=f'PR curve (AP = {ap_score:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()
```

## 3. íšŒê·€(Regression) ì„±ëŠ¥ í‰ê°€

### ê¸°ë³¸ ì§€í‘œë“¤

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression

# íšŒê·€ ë°ì´í„° ìƒì„±
X_reg = np.random.rand(1000, 3)
y_reg = 2*X_reg[:, 0] + 3*X_reg[:, 1] + X_reg[:, 2] + np.random.normal(0, 0.1, 1000)

# ëª¨ë¸ í›ˆë ¨
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3)
reg_model = LinearRegression()
reg_model.fit(X_train_reg, y_train_reg)
y_pred_reg = reg_model.predict(X_test_reg)

# í‰ê°€ ì§€í‘œë“¤
mse = mean_squared_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)

print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ²: {r2:.4f}")
```

### ì§€í‘œë³„ íŠ¹ì§•ê³¼ ì‚¬ìš© ì‹œê¸°

| ì§€í‘œ | íŠ¹ì§• | ì–¸ì œ ì‚¬ìš©í• ê¹Œ? |
|------|------|----------------|
| **MSE** | í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹° | ì´ìƒì¹˜ê°€ ì¤‘ìš”í•œ ê²½ìš° |
| **RMSE** | ì›ë³¸ ë‹¨ìœ„ë¡œ í•´ì„ ê°€ëŠ¥ | ì§ê´€ì  í•´ì„ì´ í•„ìš”í•œ ê²½ìš° |
| **MAE** | ì´ìƒì¹˜ì— robust | ì´ìƒì¹˜ê°€ ë§ì€ ë°ì´í„° |
| **RÂ²** | ì„¤ëª…ë ¥ í‘œí˜„ | ëª¨ë¸ì˜ ì „ì²´ì  ì„±ëŠ¥ íŒŒì•… |

## 4. ì‹¤ì „ íŒê³¼ ì£¼ì˜ì‚¬í•­

### ìƒí™©ë³„ ì§€í‘œ ì„ íƒ ê°€ì´ë“œ

```python
def choose_metric(problem_type, data_characteristics):
    """
    ë¬¸ì œ ìƒí™©ì— ë”°ë¥¸ ì ì ˆí•œ í‰ê°€ ì§€í‘œ ì¶”ì²œ
    """
    recommendations = {}
    
    if problem_type == "binary_classification":
        if data_characteristics["balanced"]:
            recommendations["primary"] = "Accuracy, F1-Score"
        else:
            recommendations["primary"] = "Precision, Recall, F1-Score, AUC-ROC"
            
        if data_characteristics["cost_sensitive"]:
            recommendations["secondary"] = "Precision-Recall Curve"
            
    elif problem_type == "regression":
        if data_characteristics["outliers"]:
            recommendations["primary"] = "MAE, Huber Loss"
        else:
            recommendations["primary"] = "RMSE, RÂ²"
            
    return recommendations

# ì‚¬ìš© ì˜ˆì‹œ
result = choose_metric(
    problem_type="binary_classification",
    data_characteristics={"balanced": False, "cost_sensitive": True}
)
print(result)
```

### Cross-Validationìœ¼ë¡œ ì•ˆì •ì  í‰ê°€

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')

print(f"CV F1-Scores: {cv_scores}")
print(f"í‰ê· : {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
```

## ë§ˆë¬´ë¦¬

ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ëŠ” ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ìˆ«ìë¡œ ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. **ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ**, **ë°ì´í„° íŠ¹ì„±**, **ë¹„ìš© êµ¬ì¡°**ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œëŠ” **ëª¨ë¸ í•´ì„ê³¼ ì„¤ëª… ê°€ëŠ¥í•œ AI**ì— ëŒ€í•´ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤. ì„±ëŠ¥ë„ ì¤‘ìš”í•˜ì§€ë§Œ, ì™œ ê·¸ëŸ° ì˜ˆì¸¡ì„ í–ˆëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒë„ ë§¤ìš° ì¤‘ìš”í•˜ê±°ë“ ìš”!

### í•µì‹¬ ì •ë¦¬
- **ê· í˜• ë°ì´í„°**: Accuracy, F1-Score
- **ë¶ˆê· í˜• ë°ì´í„°**: Precision, Recall, AUC-ROC
- **ë¹„ìš© ë¯¼ê°**: Precision-Recall Curve
- **íšŒê·€**: RMSE (í•´ì„ì„±) vs MAE (robustì„±)

ê¶ê¸ˆí•œ ì ì´ë‚˜ ë‹¤ë£¨ì—ˆìœ¼ë©´ í•˜ëŠ” í‰ê°€ ì§€í‘œê°€ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ“Šâœ¨
